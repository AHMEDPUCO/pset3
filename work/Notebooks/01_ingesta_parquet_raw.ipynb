{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53e30f9b-7491-497f-aa52-49b26d1248d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Snowflake context activo: DB=SPARK_DATA, SCHEMA=SPARK_DATA.RAW, WH=spark_wh, ROLE=ACCOUNTADMIN\n"
     ]
    }
   ],
   "source": [
    "from utils.snowflake_utils import get_spark_session, get_snowflake_options\n",
    "\n",
    "spark = get_spark_session(\"ingesta_raw\")\n",
    "sf_options = get_snowflake_options(schema=\"RAW\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781aaf74",
   "metadata": {},
   "source": [
    "## 0) ConfiguraciÃ³n Spark consciente del contenedor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9810a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Spark] AQE: true | shuffle.partitions: 1200\n"
     ]
    }
   ],
   "source": [
    "# --- Container-aware Spark Config (no intrusivo) ---\n",
    "# Optimiza particiones, spill y memoria para el contenedor (driver=5g, executor=3g, mem_limit=10g)\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.shuffle.targetPostShuffleInputSize\", str(64*1024*1024))  # ~64MB\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", os.getenv(\"SHUFFLE_PARTITIONS\",\"1200\"))\n",
    "spark.conf.set(\"spark.sql.shuffle.spill.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", os.getenv(\"MAX_PART_BYTES\", str(128*1024*1024)))  # 128MB\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", os.getenv(\"BROADCAST_THRESHOLD\", str(64*1024*1024)))  # 64MB\n",
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n",
    "print(\"[Spark] AQE:\", spark.conf.get(\"spark.sql.adaptive.enabled\"),\n",
    "      \"| shuffle.partitions:\", spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "001b78e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAFE MODE] SAFE_MODE=True PUT_PARALLEL=4 MAX_RSS_MB=8192 STREAM_SUMMARY=False\n"
     ]
    }
   ],
   "source": [
    "# --- SAFE MODE: estabilidad del kernel y lÃ­mites de recursos ---\n",
    "import os, time, gc\n",
    "SAFE_MODE = os.getenv(\"SAFE_MODE\", \"true\").lower() == \"true\"\n",
    "PUT_PARALLEL = int(os.getenv(\"PUT_PARALLEL\", \"4\" if SAFE_MODE else \"8\"))\n",
    "SLEEP_BETWEEN = float(os.getenv(\"SLEEP_BETWEEN\", \"1\" if SAFE_MODE else \"0\"))\n",
    "MAX_RSS_MB = int(os.getenv(\"MAX_RSS_MB\", \"8192\"))\n",
    "MAX_FAILS_ALLOWED = int(os.getenv(\"MAX_FAILS_ALLOWED\", \"5\"))\n",
    "STREAM_SUMMARY = os.getenv(\"STREAM_SUMMARY\",\"false\").lower() == \"true\"\n",
    "\n",
    "try:\n",
    "    import psutil, os as _os\n",
    "    _proc = psutil.Process(_os.getpid())\n",
    "    def current_rss_mb(): \n",
    "        return int(_proc.memory_info().rss / (1024*1024))\n",
    "except Exception:\n",
    "    def current_rss_mb(): \n",
    "        return 0\n",
    "\n",
    "def maybe_sleep():\n",
    "    if SLEEP_BETWEEN > 0:\n",
    "        time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "print(f\"[SAFE MODE] SAFE_MODE={SAFE_MODE} PUT_PARALLEL={PUT_PARALLEL} MAX_RSS_MB={MAX_RSS_MB} STREAM_SUMMARY={STREAM_SUMMARY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eca90c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helpers: retry/backoff, transacciones y monitor de memoria ---\n",
    "import time, functools, random\n",
    "\n",
    "def retry(max_tries=3, base=0.5, factor=2.0, jitter=0.1, exceptions=(Exception,)):\n",
    "    def deco(fn):\n",
    "        @functools.wraps(fn)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            delay = base\n",
    "            for attempt in range(1, max_tries+1):\n",
    "                try:\n",
    "                    return fn(*args, **kwargs)\n",
    "                except exceptions as e:\n",
    "                    if attempt == max_tries:\n",
    "                        raise\n",
    "                    sleep = delay * (1 + random.uniform(-jitter, jitter))\n",
    "                    print(f\"   â†» retry {attempt}/{max_tries-1} in {sleep:,.2f}s â†’ {e}\")\n",
    "                    time.sleep(max(0.1, sleep))\n",
    "                    delay *= factor\n",
    "        return wrapper\n",
    "    return deco\n",
    "\n",
    "class snowflake_tx:\n",
    "    def __init__(self, session):\n",
    "        self.session = session\n",
    "    def __enter__(self):\n",
    "        try:\n",
    "            self.session.sql(\"BEGIN\").collect()\n",
    "        except Exception as e:\n",
    "            print(\"BEGIN fallÃ³:\", e)\n",
    "        return self.session\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        if exc_type:\n",
    "            try:\n",
    "                self.session.sql(\"ROLLBACK\").collect()\n",
    "            except Exception as e:\n",
    "                print(\"ROLLBACK fallÃ³:\", e)\n",
    "        else:\n",
    "            try:\n",
    "                self.session.sql(\"COMMIT\").collect()\n",
    "            except Exception as e:\n",
    "                print(\"COMMIT fallÃ³:\", e)\n",
    "\n",
    "def mem_guard(prefix=\"\"):\n",
    "    try:\n",
    "        rss = current_rss_mb()\n",
    "        print(f\"{prefix} RSS={rss} MB\")\n",
    "        return rss\n",
    "    except Exception:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a062330a-8e9f-41e1-a317-de04537d1604",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", max(200, spark.sparkContext.defaultParallelism * 3))\n",
    "spark.conf.set(\"spark.default.parallelism\",     spark.sparkContext.defaultParallelism * 2)\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", 134217728)  # 128 MB por split\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\",           \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\",  \"true\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 104857600)  # 100 MB\n",
    "\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\",  \"snappy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3955a3b9-af9c-43b7-9ec0-c09e7ba7f0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDEMPOTENCY_MODE = SKIP\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# SKIP  -> si ya existe (service, year, month) en RAW, no escribe\n",
    "# REPLACE -> si ya existe, borra ese (service, year, month) y escribe de nuevo\n",
    "IDEMPOTENCY_MODE = os.environ.get(\"IDEMPOTENCY_MODE\", \"SKIP\").upper()\n",
    "assert IDEMPOTENCY_MODE in {\"SKIP\", \"REPLACE\"}, \"IDEMPOTENCY_MODE debe ser SKIP o REPLACE\"\n",
    "\n",
    "print(\"IDEMPOTENCY_MODE =\", IDEMPOTENCY_MODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10fabbc7-83b8-4ba5-8bcd-5ce683f1cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def sf_count_partition(sf_options, table, service, year, month):\n",
    "    \"\"\"Cuenta filas existentes en RAW para (service, year, month).\"\"\"\n",
    "    db = sf_options[\"sfDatabase\"]; sc = sf_options[\"sfSchema\"]\n",
    "    q = f\"\"\"\n",
    "    SELECT COUNT(*) AS CNT\n",
    "    FROM {db}.{sc}.{table}\n",
    "    WHERE service_type = '{service}'\n",
    "      AND source_year  = {year}\n",
    "      AND source_month = {month}\n",
    "    \"\"\"\n",
    "    df_cnt = (spark.read.format(\"net.snowflake.spark.snowflake\")\n",
    "              .options(**sf_options)\n",
    "              .option(\"query\", q)\n",
    "              .load())\n",
    "    return int(df_cnt.collect()[0][\"CNT\"])\n",
    "\n",
    "def sf_exec_sql(sf_options, sql):\n",
    "    \"\"\"Ejecuta un SQL DML/DDL (DELETE/MERGE/CREATE) en Snowflake vÃ­a JDBC Utils.\"\"\"\n",
    "    jvm = spark._jvm\n",
    "    jmap = jvm.java.util.HashMap()\n",
    "    for k, v in sf_options.items():\n",
    "        jmap.put(k, v)\n",
    "    jvm.net.snowflake.spark.snowflake.Utils.runQuery(jmap, sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f915eeca-5a1f-4458-8696-61811271b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evitar duplicados dentro del lote\n",
    "NATURAL_KEY = [\n",
    "    \"pickup_datetime\", \"dropoff_datetime\",   \n",
    "    \"PULocationID\", \"DOLocationID\",\n",
    "    \"VendorID\"\n",
    "]\n",
    "\n",
    "def drop_dupes_by_key(df, key_cols):\n",
    "    present = [c for c in key_cols if c in df.columns]\n",
    "    if not present:\n",
    "        return df  \n",
    "    return df.dropDuplicates(present)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2a1b8bd3-0e64-4d04-851c-510e5874674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParÃ¡metros:\n",
      " service   = green\n",
      " year      = 2022\n",
      " months    = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      " run_id    = 984827f6-9d04-40cd-a60d-1ef0e61fca24\n",
      " base_url  = https://d37ci6vzurychx.cloudfront.net/trip-data\n",
      " raw_table = RAW_TLC_TRIPS_green\n",
      " data_dir  = /home/jovyan/work/data\n"
     ]
    }
   ],
   "source": [
    "import os, uuid, datetime, subprocess, sys\n",
    "from pathlib import Path\n",
    "\n",
    "# --- ParÃ¡metros---\n",
    "SERVICE   = os.environ.get(\"DM_SERVICE\", \"green\")      # \"yellow\" o \"green\"\n",
    "YEAR      = os.environ.get(\"DM_YEAR\", \"2022\")\n",
    "months_str = os.environ.get(\"DM_MONTHS\", \"1,2,3\")  # default a smoke test\n",
    "MONTHS = [int(m.strip()) for m in months_str.split(\",\")]\n",
    "RUN_ID    = os.environ.get(\"RUN_ID\", str(uuid.uuid4()))\n",
    "BASE_URL  = os.environ.get(\"TLC_BASE_URL\", \"https://d37ci6vzurychx.cloudfront.net/trip-data\")\n",
    "RAW_TABLE = os.environ.get(\"RAW_TABLE\", \"RAW_TLC_TRIPS_green\")  # RAW_TLC_TRIPS_yellow\n",
    "\n",
    "# Ruta de trabajo dentro del contenedor Jupyter\n",
    "DATA_DIR = Path(\"/home/jovyan/work/data\")  # respeta el volumen del docker-compose\n",
    "EVID_DIR = Path(\"/home/jovyan/work/evidencias\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EVID_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ParÃ¡metros:\")\n",
    "print(\" service   =\", SERVICE)\n",
    "print(\" year      =\", YEAR)\n",
    "print(\" months    =\", MONTHS)\n",
    "print(\" run_id    =\", RUN_ID)\n",
    "print(\" base_url  =\", BASE_URL)\n",
    "print(\" raw_table =\", RAW_TABLE)\n",
    "print(\" data_dir  =\", DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2e39171e-57d6-4efc-bd99-72d84b8fb5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando green_tripdata_2022-01.parquet\n",
      "â†’ wget -nc -P /home/jovyan/work/data https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-01.parquet\n",
      "Descargando green_tripdata_2022-02.parquet\n",
      "â†’ wget -nc -P /home/jovyan/work/data https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-02.parquet\n",
      "Descargando green_tripdata_2022-03.parquet\n",
      "â†’ wget -nc -P /home/jovyan/work/data https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-03.parquet\n",
      "Descargando green_tripdata_2022-04.parquet\n",
      "â†’ wget -nc -P /home/jovyan/work/data https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-04.parquet\n",
      "Descargando green_tripdata_2022-05.parquet\n",
      "â†’ wget -nc -P /home/jovyan/work/data https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-05.parquet\n",
      "Descargando green_tripdata_2022-06.parquet\n",
      "â†’ wget -nc -P /home/jovyan/work/data https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-06.parquet\n",
      "Descargando green_tripdata_2022-07.parquet\n",
      "â†’ wget -nc -P /home/jovyan/work/data https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-07.parquet\n",
      "Descargando green_tripdata_2022-08.parquet\n",
      "â†’ wget -nc -P /home/jovyan/work/data https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-08.parquet\n",
      "Descargando green_tripdata_2022-09.parquet\n",
      "â†’ wget -nc -P /home/jovyan/work/data https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-09.parquet\n",
      "Descargando green_tripdata_2022-10.parquet\n",
      "â†’ wget -nc -P /home/jovyan/work/data https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-10.parquet\n",
      "Descargando green_tripdata_2022-11.parquet\n",
      "â†’ wget -nc -P /home/jovyan/work/data https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-11.parquet\n",
      "Descargando green_tripdata_2022-12.parquet\n",
      "â†’ wget -nc -P /home/jovyan/work/data https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2022-12.parquet\n"
     ]
    }
   ],
   "source": [
    "def wget(url: str, out_dir: Path):\n",
    "    # -nc (no clobber): no vuelve a descargar si ya existe\n",
    "    # -P  : directorio de salida\n",
    "    cmd = [\"wget\", \"-nc\", \"-P\", str(out_dir), url]\n",
    "    print(\"â†’\", \" \".join(cmd))\n",
    "    return subprocess.call(cmd)\n",
    "\n",
    "for m in MONTHS:\n",
    "    ym = f\"{YEAR}-{m:02d}\"\n",
    "    file_name = f\"{SERVICE}_tripdata_{ym}.parquet\"\n",
    "    url = f\"{BASE_URL}/{file_name}\"\n",
    "    print(f\"Descargando {file_name}\")\n",
    "    rc = wget(url, DATA_DIR)\n",
    "    if rc not in (0, 8):  # 0 = OK, 8 = already exists (wget -nc)\n",
    "        print(f\"[WARN] wget devolviÃ³ cÃ³digo {rc} para {url}\", file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b6683d8d-54fd-467d-98f9-97d2439190ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Procesando green_tripdata_2022-01.parquet ===\n",
      "â†ª SKIP: Ya hay 62495 filas en RAW para (green, 2022-01). Saltando este mes.\n",
      "\n",
      "=== Procesando green_tripdata_2022-02.parquet ===\n",
      "â†ª SKIP: Ya hay 69399 filas en RAW para (green, 2022-02). Saltando este mes.\n",
      "\n",
      "=== Procesando green_tripdata_2022-03.parquet ===\n",
      "â†ª SKIP: Ya hay 78537 filas en RAW para (green, 2022-03). Saltando este mes.\n",
      "\n",
      "=== Procesando green_tripdata_2022-04.parquet ===\n",
      "Conteo (acciÃ³n â†’ visible en Spark UI): 75962\n",
      "\n",
      "=== Procesando green_tripdata_2022-05.parquet ===\n",
      "Conteo (acciÃ³n â†’ visible en Spark UI): 76725\n",
      "\n",
      "=== Procesando green_tripdata_2022-06.parquet ===\n",
      "Conteo (acciÃ³n â†’ visible en Spark UI): 73534\n",
      "\n",
      "=== Procesando green_tripdata_2022-07.parquet ===\n",
      "â†ª SKIP: Ya hay 64038 filas en RAW para (green, 2022-07). Saltando este mes.\n",
      "\n",
      "=== Procesando green_tripdata_2022-08.parquet ===\n",
      "â†ª SKIP: Ya hay 65776 filas en RAW para (green, 2022-08). Saltando este mes.\n",
      "\n",
      "=== Procesando green_tripdata_2022-09.parquet ===\n",
      "Conteo (acciÃ³n â†’ visible en Spark UI): 68855\n",
      "\n",
      "=== Procesando green_tripdata_2022-10.parquet ===\n",
      "Conteo (acciÃ³n â†’ visible en Spark UI): 69178\n",
      "\n",
      "=== Procesando green_tripdata_2022-11.parquet ===\n",
      "Conteo (acciÃ³n â†’ visible en Spark UI): 62152\n",
      "\n",
      "=== Procesando green_tripdata_2022-12.parquet ===\n",
      "Conteo (acciÃ³n â†’ visible en Spark UI): 72224\n",
      "\n",
      "=== Total filas combinadas: 498630 ===\n",
      "\n",
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: integer (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: double (nullable = true)\n",
      " |-- trip_type: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- run_id: string (nullable = false)\n",
      " |-- service_type: string (nullable = false)\n",
      " |-- source_year: string (nullable = false)\n",
      " |-- source_month: integer (nullable = false)\n",
      " |-- ingested_at_utc: string (nullable = false)\n",
      " |-- source_path: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "dfs = []\n",
    "for m in MONTHS:\n",
    "    ym = f\"{YEAR}-{m:02d}\"\n",
    "    file_name = f\"{SERVICE}_tripdata_{ym}.parquet\"\n",
    "    local_path = str(DATA_DIR / file_name)\n",
    "    source_url = f\"{BASE_URL}/{file_name}\"\n",
    "\n",
    "    print(f\"\\n=== Procesando {file_name} ===\")\n",
    "\n",
    "    # 4.1) Idempotencia: Â¿ya existe ese particionado en RAW?\n",
    "    try:\n",
    "        existing = sf_count_partition(sf_options, RAW_TABLE, SERVICE, YEAR, m)\n",
    "    except Exception as e:\n",
    "        existing = 0\n",
    "        print(\"[INFO] No pude contar en RAW (Â¿sin credenciales?). Continuo como si no existiera. Detalle:\", e)\n",
    "\n",
    "    if existing > 0 and IDEMPOTENCY_MODE == \"SKIP\":\n",
    "        print(f\"â†ª SKIP: Ya hay {existing} filas en RAW para ({SERVICE}, {YEAR}-{m:02d}). Saltando este mes.\")\n",
    "        continue\n",
    "\n",
    "    if existing > 0 and IDEMPOTENCY_MODE == \"REPLACE\":\n",
    "        db = sf_options[\"sfDatabase\"]; sc = sf_options[\"sfSchema\"]\n",
    "        sql_del = f\"\"\"\n",
    "        DELETE FROM {db}.{sc}.{RAW_TABLE}\n",
    "        WHERE service_type = '{SERVICE}' AND source_year = {YEAR} AND source_month = {m}\n",
    "        \"\"\"\n",
    "        print(f\"â†ª REPLACE: Borrando particiÃ³n existente en RAW ({SERVICE}, {YEAR}-{m:02d})â€¦\")\n",
    "        try:\n",
    "            sf_exec_sql(sf_options, sql_del)\n",
    "            print(\"   âœ“ DELETE completado\")\n",
    "        except Exception as e:\n",
    "            print(\"   [WARN] No se pudo borrar. Detalle:\", e)\n",
    "\n",
    "    # 4.2) Lee local si existe; si no, directo por URL\n",
    "    path_to_read = local_path if Path(local_path).exists() else source_url\n",
    "    df = spark.read.parquet(path_to_read)\n",
    "\n",
    "    # 4.3) Normaliza timestamps (y opcionalmente elimina columnas originales para evitar NTZ)\n",
    "    pickup_col  = \"tpep_pickup_datetime\"  if SERVICE == \"yellow\" else \"lpep_pickup_datetime\"\n",
    "    dropoff_col = \"tpep_dropoff_datetime\" if SERVICE == \"yellow\" else \"lpep_dropoff_datetime\"\n",
    "\n",
    "    df_std = (\n",
    "        df\n",
    "        .withColumn(\"pickup_datetime\",  F.col(pickup_col).cast(\"timestamp\"))\n",
    "        .withColumn(\"dropoff_datetime\", F.col(dropoff_col).cast(\"timestamp\"))\n",
    "        .drop(pickup_col, dropoff_col)  # evita choques de tipos al escribir\n",
    "    )\n",
    "\n",
    "    # 4.4) Metadatos obligatorios\n",
    "    df_enriched = (\n",
    "        df_std\n",
    "        .withColumn(\"run_id\",          F.lit(RUN_ID))\n",
    "        .withColumn(\"service_type\",    F.lit(SERVICE))\n",
    "        .withColumn(\"source_year\",     F.lit(YEAR))\n",
    "        .withColumn(\"source_month\",    F.lit(m))\n",
    "        .withColumn(\"ingested_at_utc\", F.lit(datetime.datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"))\n",
    "        .withColumn(\"source_path\",     F.lit(source_url))\n",
    "    )\n",
    "\n",
    "    # 4.5) DeduplicaciÃ³n intra-lote por clave natural\n",
    "    df_enriched = drop_dupes_by_key(df_enriched, NATURAL_KEY)\n",
    "\n",
    "    # 4.6) AcciÃ³n para Spark UI (conteo) y acumulaciÃ³n\n",
    "    print(\"Conteo (acciÃ³n â†’ visible en Spark UI):\", df_enriched.count())\n",
    "    dfs.append(df_enriched)\n",
    "\n",
    "# 4.7) UniÃ³n segura por nombre\n",
    "if not dfs:\n",
    "    print(\"No hubo nada que escribir (todos los meses estaban ya cargados o fallÃ³ la carga).\")\n",
    "else:\n",
    "    df_all = dfs[0]\n",
    "    for extra in dfs[1:]:\n",
    "        df_all = df_all.unionByName(extra, allowMissingColumns=True)\n",
    "\n",
    "    print(\"\\n=== Total filas combinadas:\", df_all.count(), \"===\\n\")\n",
    "    df_all.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "03f82304-f2b6-493e-9c07-69bc6c145957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escritura a snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4e350107-b4e8-415c-826a-ed7bf73afeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Escritura completada en RAW â†’ RAW_TLC_TRIPS_green\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Reparticiona para distribuir carga en memoria\n",
    "    df_all = df_all.repartition(16)  # puedes ajustar el nÃºmero segÃºn recursos disponibles\n",
    "\n",
    "    # Escritura a Snowflake\n",
    "    (df_all.write\n",
    "     .format(\"net.snowflake.spark.snowflake\")\n",
    "     .options(**sf_options)\n",
    "     .option(\"dbtable\", RAW_TABLE)\n",
    "     .option(\"parallelism\", os.getenv(\"SNOWFLAKE_PARALLELISM\", \"16\"))  # mejora throughput\n",
    "     # .option(\"onError\", \"CONTINUE\")  # opcional para tolerancia a filas malas\n",
    "     .mode(\"append\")  # append es seguro por idempotencia aplicada previamente\n",
    "     .save())\n",
    "\n",
    "    print(f\"âœ“ Escritura completada en RAW â†’ {RAW_TABLE}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"[INFO] AÃºn no escribimos en Snowflake (probable falta de credenciales, permisos o memoria).\")\n",
    "    print(\"Detalle:\", e)\n",
    "\n",
    "    # ðŸ§  Tip adicional de diagnÃ³stico\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e99d2eb2-75ac-45b0-a593-8f9c70abfb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ AuditorÃ­a actualizada â†’ /home/jovyan/work/evidencias/audit_ingesta_raw.csv\n"
     ]
    }
   ],
   "source": [
    "import csv, datetime\n",
    "audit_csv = EVID_DIR / \"audit_ingesta_raw.csv\"\n",
    "headers = [\"run_id\",\"service_type\",\"source_year\",\"source_months\",\"rows\",\"when_utc\",\"raw_table\",\"mode\"]\n",
    "\n",
    "if dfs:\n",
    "    row = [RUN_ID, SERVICE, YEAR, \",\".join(map(str, MONTHS)),\n",
    "           df_all.count(),\n",
    "           datetime.datetime.utcnow().isoformat(timespec=\"seconds\")+\"Z\",\n",
    "           RAW_TABLE, IDEMPOTENCY_MODE]\n",
    "    write_headers = not audit_csv.exists()\n",
    "    with open(audit_csv, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        if write_headers: w.writerow(headers)\n",
    "        w.writerow(row)\n",
    "    print(\"âœ“ AuditorÃ­a actualizada â†’\", str(audit_csv))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
